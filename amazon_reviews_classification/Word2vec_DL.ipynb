{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing Libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nimport warnings\nimport spacy\nimport re\nimport string\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\nwarnings.filterwarnings('ignore') # Hides warning\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\",category=UserWarning)\nplt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\") # Plotting style\nnp.random.seed(10) # seeding random number generator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuring path\ndata_path = '../input/amazon-customerreviews-polarity'\ntrain_data_path = data_path + '/train.csv'\ntest_data_path = data_path + '/test.csv'\noutput_path = '../working/'\nmodel_path = output_path + 'model/'\noutput_file_path = output_path + 'file/'\n!mkdir \"$model_path\"\n!mkdir \"$output_file_path\"\ncleaned_train_input_file_path = output_file_path + '/train_clean.csv'\ncleaned_test_input_file_path = output_file_path + '/test_clean.csv'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loading\ntrain_df = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)\n\n# column addition\ntrain_df.columns = ['polarity','title','text']\ntest_df.columns = ['polarity','title','text']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to convert score to sentiment\ndef to_sentiment(rating):\n    \n    rating = int(rating)\n    \n    # Convert to class\n    if rating == 1: # negative\n        return 0\n    elif rating == 2: # Positive\n        return 1\n    else:\n        return 3\n\n# Apply to the dataset \ntrain_df['polarity'] = train_df.polarity.apply(to_sentiment)\ntest_df['polarity'] = test_df.polarity.apply(to_sentiment)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing label counts\ntrain_df['polarity'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check for missing values in train data\nprint('---------Training Data-----------')\nprint(train_df.isnull().sum())\n\n# Let's check for missing values in test data\nprint('---------Test Data-----------')\nprint(test_df.isnull().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Substituting Null values with empty spaces\ntrain_df['title'] = train_df['title'].fillna(' ')\ntest_df['title'] = test_df['title'].fillna(' ')\n\n# Let's check for missing values in train data\nprint('---------Training Data-----------')\nprint(train_df.isnull().sum())\n\n# Let's check for missing values in test data\nprint('---------Test Data-----------')\nprint(test_df.isnull().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['review'] = train_df['title'].astype(str) + ' ' + train_df['text'].astype(str)\ntest_df['review'] = test_df['title'].astype(str) + ' ' + test_df['text'].astype(str)\n\nprint('---------Training Data Shape-----------')\nprint(train_df.shape)\n\nprint('---------Test Data Shape-----------')\nprint(test_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 10K elements from your dataframe\ntrain_df = train_df.sample(n=50000)\ntest_df = test_df.sample(n=500)\nprint(train_df.shape,train_df.shape)\nprint(test_df.shape,test_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Check if the text is a string\n    if not isinstance(text, str):\n        return []\n    \n    # Keep only letters and whitespaces\n    pattern = f\"[a-zA-Z\\s]\"\n    text = ''.join(re.findall(pattern, text))\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n    \n    return tokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply the preprocess text to \ntrain_df['review'] = train_df['review'].apply(preprocess_text)\ntest_df['review'] = test_df['review'].apply(preprocess_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetch embeddings\nword2vec_model = Word2Vec(sentences=train_df.review.values.tolist(), \n                          vector_size=100, min_count=1, workers=4)\n\n# Get vocabulary size\nvocab_size = len(word2vec_model.wv)\nprint(vocab_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert text to Word2Vec embeddings\ndef text_to_embeddings(text, word2vec_model, seq_length):\n    embeddings = []\n    \n    for i, word in enumerate(text):\n        if word in word2vec_model.wv:\n            if i == seq_length:\n                break\n            embeddings.append(word2vec_model.wv[word])\n        else:\n            continue\n        \n    # Padding the sequences\n    if len(embeddings) < seq_length:\n        zero_padding = [np.zeros(word2vec_model.vector_size) \\\n                        for _ in range(seq_length - len(embeddings))]\n\n        embeddings = embeddings + zero_padding\n\n    return embeddings[:seq_length]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data\ndef prepare_data(reviews, labels, word2vec_model):\n    X = [text_to_embeddings(review, word2vec_model, 100) for review in reviews]\n    X = [torch.tensor(embeddings, dtype=torch.float32) for embeddings in X]\n    y = torch.tensor(labels, dtype=torch.long)\n    return X, y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separating out independent and dependent columns\nX_train = train_df.drop(['polarity'],axis = 1)\nY_train = train_df.drop(['title','text','review'],axis = 1)\nX_test = test_df.drop(['polarity'],axis = 1)\nY_test = test_df.drop(['title','text','review'],axis = 1)\n\nX_train = X_train.drop(['title','text'],axis = 1)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.33, random_state=4, stratify=Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data\nX_train, y_train = prepare_data(X_train.review, Y_train.polarity.values,\n                    word2vec_model)\n\nX_val, y_val = prepare_data(X_val.review, Y_val.polarity.values,\n                    word2vec_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN Model ","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\ninput_size = word2vec_model.vector_size\nhidden_size = 128\noutput_size = 1\nnum_layers = 1\nlearning_rate = 0.001\nnum_epochs = 30\nbatch_size = 64\ndropout_rate = 0.5\nleaky_relu_slope = 0.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataLoader\ntrain_data = TensorDataset(torch.stack(X_train), y_train)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataLoader\nval_data = TensorDataset(torch.stack(X_val), y_val)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n        super(SentimentRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        # Basic RNN layer\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n        self.leaky_relu = nn.LeakyReLU()  # Leaky ReLU activation layer\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()  # Sigmoid activation layer\n\n    def forward(self, x):\n        # Initial hidden state\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        # RNN output\n        out, _ = self.rnn(x, h0)\n        # Apply Leaky ReLU to the outputs of the RNN layer\n        out = self.leaky_relu(out)\n        out = self.dropout(out)\n        # Get the last sequence output for classification\n        out = out[:, -1, :]\n        # Apply the linear layer for the final output\n        out = self.fc(out)\n        # Apply the sigmoid activation\n        out = self.sigmoid(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\nmodel = SentimentRNN(input_size, hidden_size, output_size, dropout_rate)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            outputs = model(inputs)\n            predicted = outputs.squeeze() > 0.5\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training loop\nimport matplotlib.pyplot as plt\n\nnum_epochs = 30  # Number of epochs\nlosses = []  # List to store the average train loss per epoch\nval_losses = []  # List to store the average validation loss per epoch\nbest_val_loss = float('inf')  # Initialize the best validation loss to infinity\nbest_epoch = 0  # Epoch with the best validation loss\npatience = 0\nmax_patience = 3  # Maximum epochs to wait for improvement\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    total_val_loss = 0\n    count = 0\n    val_count = 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        outputs = outputs.squeeze()\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        count += 1\n    average_loss = total_loss / count\n    losses.append(average_loss)\n\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            val_outputs = model(inputs)\n            val_outputs = val_outputs.squeeze()\n            val_loss = criterion(val_outputs, labels.float())\n            total_val_loss += val_loss.item()\n            val_count += 1\n    average_val_loss = total_val_loss / val_count\n    val_losses.append(average_val_loss)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')\n    \n    # Check if the current validation loss is the lowest; if so, save the model\n    if average_val_loss < best_val_loss:\n        best_val_loss = average_val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), model_path + 'rnn_best_model.pth')  # Save the best model\n        patience = 0   \n    else:\n        patience += 1\n\n    if patience >= max_patience:\n        print(f'Early stopped at {epoch+1}')\n        break  # Stop training\n\nprint(f'Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}')\n\n# Plotting the training and validation losses\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, (len(losses)+1)), losses, 'bo-', label='Training Loss')\nplt.plot(range(1, (len(losses)+1)), val_losses, 'ro-', label='Validation Loss')\nplt.title('Training and Validation Loss per Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model and calculate accuracy only for that\nmodel.load_state_dict(torch.load(model_path + 'rnn_best_model.pth'))\ntrain_accuracy = calculate_accuracy(train_loader)\nval_accuracy = calculate_accuracy(val_loader)\nprint(f'Best Model Training Accuracy: {train_accuracy}%')\nprint(f'Best Model Validation Accuracy: {val_accuracy}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi-Directional RNN","metadata":{}},{"cell_type":"code","source":"# Define num_layers\nnum_layers = 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentBiRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate=0.5):\n        super(SentimentBiRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # Using RNN for the recurrent layer\n        self.bi_rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True,\n                             bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0)\n        self.dropout = nn.Dropout(dropout_rate)\n        # Doubling the output size because of bidirectionality\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, x):\n        # Initialize hidden state for RNN\n        h0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        # Forward propagate the RNN\n        out, _ = self.bi_rnn(x, h0)\n        # Apply Leaky ReLU activation\n        out = nn.functional.leaky_relu(out)\n        out = self.dropout(out)\n        # Get the last time step's output for the fully connected layer\n        out = self.fc(out[:, -1, :])\n        # Apply sigmoid to the output layer\n        out = torch.sigmoid(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\nmodel = SentimentBiRNN(input_size, hidden_size, output_size, num_layers, dropout_rate)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training loop\nimport matplotlib.pyplot as plt\n\nnum_epochs = 30  # Number of epochs\nlosses = []  # List to store the average train loss per epoch\nval_losses = []  # List to store the average validation loss per epoch\nbest_val_loss = float('inf')  # Initialize the best validation loss to infinity\nbest_epoch = 0  # Epoch with the best validation loss\npatience = 0\nmax_patience = 3  # Maximum epochs to wait for improvement\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    total_val_loss = 0\n    count = 0\n    val_count = 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        outputs = outputs.squeeze()\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        count += 1\n    average_loss = total_loss / count\n    losses.append(average_loss)\n\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            val_outputs = model(inputs)\n            val_outputs = val_outputs.squeeze()\n            val_loss = criterion(val_outputs, labels.float())\n            total_val_loss += val_loss.item()\n            val_count += 1\n    average_val_loss = total_val_loss / val_count\n    val_losses.append(average_val_loss)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')\n    \n    # Check if the current validation loss is the lowest; if so, save the model\n    if average_val_loss < best_val_loss:\n        best_val_loss = average_val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), model_path + 'bidirectional_best_model.pth')  # Save the best model\n        patience = 0   \n    else:\n        patience += 1\n\n    if patience >= max_patience:\n        print(f'Early stopped at {epoch+1}')\n        break  # Stop training\n\nprint(f'Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}')\n\n# Plotting the training and validation losses\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, (len(losses)+1)), losses, 'bo-', label='Training Loss')\nplt.plot(range(1, (len(losses)+1)), val_losses, 'ro-', label='Validation Loss')\nplt.title('Training and Validation Loss per Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model and calculate accuracy only for that\nmodel.load_state_dict(torch.load(model_path + 'bidirectional_best_model.pth'))\ntrain_accuracy = calculate_accuracy(train_loader)\nval_accuracy = calculate_accuracy(val_loader)\nprint(f'Best Model Training Accuracy: {train_accuracy}%')\nprint(f'Best Model Validation Accuracy: {val_accuracy}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi- Directional GRU","metadata":{}},{"cell_type":"code","source":"class SentimentGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate=0.5, leaky_relu_slope=0.1):\n        super(SentimentGRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, \n                          bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # Output size is doubled because of bidirectionality\n    \n    def forward(self, x):\n        # Initialize hidden state\n        h0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        # Forward propagate the GRU\n        out, _ = self.gru(x, h0)\n        # Apply Leaky ReLU activation\n        out = nn.functional.leaky_relu(out)\n        # Apply dropout\n        out = self.dropout(out)\n        # Use only the last output for the fully connected layer\n        out = self.fc(out[:, -1, :])\n        # Apply sigmoid to the output layer\n        out = torch.sigmoid(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\nmodel = SentimentGRU(input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training loop\nimport matplotlib.pyplot as plt\n\nnum_epochs = 30  # Number of epochs\nlosses = []  # List to store the average train loss per epoch\nval_losses = []  # List to store the average validation loss per epoch\nbest_val_loss = float('inf')  # Initialize the best validation loss to infinity\nbest_epoch = 0  # Epoch with the best validation loss\npatience = 0\nmax_patience = 3  # Maximum epochs to wait for improvement\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    total_val_loss = 0\n    count = 0\n    val_count = 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        outputs = outputs.squeeze()\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        count += 1\n    average_loss = total_loss / count\n    losses.append(average_loss)\n\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            val_outputs = model(inputs)\n            val_outputs = val_outputs.squeeze()\n            val_loss = criterion(val_outputs, labels.float())\n            total_val_loss += val_loss.item()\n            val_count += 1\n    average_val_loss = total_val_loss / val_count\n    val_losses.append(average_val_loss)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')\n    \n    # Check if the current validation loss is the lowest; if so, save the model\n    if average_val_loss < best_val_loss:\n        best_val_loss = average_val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), model_path + 'bi_gru_best_model.pth')  # Save the best model\n        patience = 0   \n    else:\n        patience += 1\n\n    if patience >= max_patience:\n        print(f'Early stopped at {epoch+1}')\n        break  # Stop training\n\nprint(f'Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}')\n\n# Plotting the training and validation losses\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, (len(losses)+1)), losses, 'bo-', label='Training Loss')\nplt.plot(range(1, (len(losses)+1)), val_losses, 'ro-', label='Validation Loss')\nplt.title('Training and Validation Loss per Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model and calculate accuracy only for that\nmodel.load_state_dict(torch.load( model_path + 'bi_gru_best_model.pth'))\ntrain_accuracy = calculate_accuracy(train_loader)\nval_accuracy = calculate_accuracy(val_loader)\nprint(f'Best Model Training Accuracy: {train_accuracy}%')\nprint(f'Best Model Validation Accuracy: {val_accuracy}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi-Directional LSTM","metadata":{}},{"cell_type":"code","source":"class SentimentLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate=0.5, leaky_relu_slope=0.1):\n        super(SentimentLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.leaky_relu_slope = leaky_relu_slope\n\n        # # Create LSTM layers with nn.LSTM\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        \n        # Define dropout layer\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Fully connected layer (output size is doubled for bidirectionality)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n        # Sigmoid activation for output layer\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        h_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n\n        outputs, _ = self.lstm(x, (h_0, c_0))\n        \n        outputs = F.leaky_relu(outputs, negative_slope=self.leaky_relu_slope)  # Leaky ReLU activation\n\n        # Apply dropout and fully connected layer\n        outputs = self.dropout(outputs)\n        out = self.fc(outputs[:, -1, :])\n\n        # Sigmoid activation for output\n        out = self.sigmoid(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\nmodel = SentimentLSTM(input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training loop\nimport matplotlib.pyplot as plt\n\nnum_epochs = 30  # Number of epochs\nlosses = []  # List to store the average train loss per epoch\nval_losses = []  # List to store the average validation loss per epoch\nbest_val_loss = float('inf')  # Initialize the best validation loss to infinity\nbest_epoch = 0  # Epoch with the best validation loss\npatience = 0\nmax_patience = 3  # Maximum epochs to wait for improvement\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n    total_val_loss = 0\n    count = 0\n    val_count = 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        outputs = outputs.squeeze()\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        count += 1\n    average_loss = total_loss / count\n    losses.append(average_loss)\n\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            val_outputs = model(inputs)\n            val_outputs = val_outputs.squeeze()\n            val_loss = criterion(val_outputs, labels.float())\n            total_val_loss += val_loss.item()\n            val_count += 1\n    average_val_loss = total_val_loss / val_count\n    val_losses.append(average_val_loss)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')\n    \n    # Check if the current validation loss is the lowest; if so, save the model\n    if average_val_loss < best_val_loss:\n        best_val_loss = average_val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), model_path + 'bi_lstm_best_model.pth')  # Save the best model\n        patience = 0   \n    else:\n        patience += 1\n\n    if patience >= max_patience:\n        print(f'Early stopped at {epoch+1}')\n        break  # Stop training\n\nprint(f'Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}')\n\n# Plotting the training and validation losses\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, (len(losses)+1)), losses, 'bo-', label='Training Loss')\nplt.plot(range(1, (len(losses)+1)), val_losses, 'ro-', label='Validation Loss')\nplt.title('Training and Validation Loss per Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model and calculate accuracy only for that\nmodel.load_state_dict(torch.load(model_path + 'bi_lstm_best_model.pth'))\ntrain_accuracy = calculate_accuracy(train_loader)\nval_accuracy = calculate_accuracy(val_loader)\nprint(f'Best Model Training Accuracy: {train_accuracy}%')\nprint(f'Best Model Validation Accuracy: {val_accuracy}%')","metadata":{},"execution_count":null,"outputs":[]}]}