{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification using LoRA"
      ],
      "metadata": {
        "id": "v8x8WDQ9K6QO"
      },
      "id": "v8x8WDQ9K6QO"
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import os\n",
        "\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, default_data_collator, get_linear_schedule_with_warmup, TrainingArguments\n",
        "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
        "from peft import get_peft_config, get_peft_model, get_peft_model, LoraConfig, TaskType, PeftModel, PeftType, PeftConfig\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from trl import SFTTrainer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ET0sd0j1Gsd6"
      },
      "id": "ET0sd0j1Gsd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9856e4fa-8760-4f0a-aeca-32db04e8928d",
      "metadata": {
        "id": "9856e4fa-8760-4f0a-aeca-32db04e8928d"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"lora_learning_methods\", name=\"lora\")\n",
        "seed = 42\n",
        "device = \"cuda\"\n",
        "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
        "text_column = \"input\"\n",
        "label_column = \"output\"\n",
        "max_length = 64\n",
        "lr = 1e-4\n",
        "num_epochs = 10\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fcd09d8-638d-4038-b365-a67359ddd79e",
      "metadata": {
        "id": "7fcd09d8-638d-4038-b365-a67359ddd79e"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "dataset = load_dataset(\"FinGPT/fingpt-sentiment-train\")\n",
        "\n",
        "classes = list(set([k[\"output\"] for k in dataset[\"train\"]]))\n",
        "print(dataset)\n",
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "4NYNpRURfa8T"
      },
      "id": "4NYNpRURfa8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset splitting\n",
        "train_testvalid = dataset['train'].train_test_split(test_size=0.2)\n",
        "# Split the 10% test + valid in half test, half valid\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "# gather everyone if you want to have a single DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})"
      ],
      "metadata": {
        "id": "kVDogPhi_sMw"
      },
      "id": "kVDogPhi_sMw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path) #token=hf_token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
        "print(f\"{target_max_length=}\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    batch_size = len(examples[text_column]) # define batch size, here it is no. of rows\n",
        "    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]] # format input as Input text: input Label:\n",
        "    targets = [str(x) for x in examples[label_column]] # get the labels as targets\n",
        "\n",
        "    # tokenize input and label both\n",
        "    model_inputs = tokenizer(inputs)\n",
        "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i] # get the inputs\n",
        "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id] # get the label\n",
        "        # print(i, sample_input_ids, label_input_ids)\n",
        "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids # add label to the input\n",
        "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids # make the label length same as input\n",
        "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i]) # make attention mask of same length\n",
        "\n",
        "    # Each example in the dataset is tokenized but they have different lengths. The next for loop makes all of\n",
        "    # them equal in length by adding tokenizer.pad_token_id at the beginning. Similarly we add 0s at the\n",
        "    # beginning of attention masks and -100 for labels.\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        label_input_ids = labels[\"input_ids\"][i]\n",
        "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
        "            max_length - len(sample_input_ids)\n",
        "        ) + sample_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
        "            \"attention_mask\"\n",
        "        ][i]\n",
        "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
        "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
        "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
        "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "eval_dataset = dataset[\"valid\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
        ")\n",
        "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "nRoc4okf1eHT"
      },
      "id": "nRoc4okf1eHT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing test dataset\n",
        "def test_preprocess_function(examples):\n",
        "    batch_size = len(examples[text_column])\n",
        "    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n",
        "    model_inputs = tokenizer(inputs)\n",
        "    # print(model_inputs)\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
        "            max_length - len(sample_input_ids)\n",
        "        ) + sample_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
        "            \"attention_mask\"\n",
        "        ][i]\n",
        "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
        "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "test_dataset = dataset[\"test\"].map(\n",
        "    test_preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
        "next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "aNvYnDaLC4O1"
      },
      "id": "aNvYnDaLC4O1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the PEFT model, Optimizer and LR Scheduler\n"
      ],
      "metadata": {
        "id": "syL6L-vyIG2F"
      },
      "id": "syL6L-vyIG2F"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(r=8,\n",
        "                         lora_alpha=16,\n",
        "                         lora_dropout=0.1,\n",
        "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"],\n",
        "                         task_type=TaskType.CAUSAL_LM)"
      ],
      "metadata": {
        "id": "owijwU7LIcMm"
      },
      "id": "owijwU7LIcMm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# cast non-trainable params in fp16\n",
        "for p in model.parameters():\n",
        "    if not p.requires_grad:\n",
        "        p.data = p.to(torch.float16)"
      ],
      "metadata": {
        "id": "r-oS7_IhIlx-"
      },
      "id": "r-oS7_IhIlx-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "HkfE0JI6IxDm"
      },
      "id": "HkfE0JI6IxDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WlnI_K6bJAxi"
      },
      "id": "WlnI_K6bJAxi"
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"mistral_lora_sentiment\"\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 8\n",
        "logging_steps = 5\n",
        "learning_rate = 5e-4\n",
        "max_grad_norm = 1.0\n",
        "max_steps = 250\n",
        "num_train_epochs=10\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    fp16=True,\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    hub_private_repo=True,\n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")"
      ],
      "metadata": {
        "id": "TZwkrooXIy5H"
      },
      "id": "TZwkrooXIy5H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    # packing=True,\n",
        "    # dataset_text_field=\"content\",\n",
        "    # max_seq_length=max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "id": "LrpMWs3hJXBY"
      },
      "id": "LrpMWs3hJXBY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "_BBY8BKKJocu"
      },
      "id": "_BBY8BKKJocu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "iIgmmrtyJrt3"
      },
      "id": "iIgmmrtyJrt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "Ek_nEsXQJwVh"
      },
      "id": "Ek_nEsXQJwVh"
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the trained model and getting the predictions of the trained model\n",
        "peft_model_id = \"anurag-kr/mistral_lora_sentiment\"\n",
        "device = \"cuda\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
      ],
      "metadata": {
        "id": "HwDU-1gGJ1oI"
      },
      "id": "HwDU-1gGJ1oI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "i = 36\n",
        "inputs = tokenizer(f'{text_column} : {test_dataset[i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n",
        "# print(test_dataset[i][\"input\"])\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "MfWUtQi0MgGc"
      },
      "id": "MfWUtQi0MgGc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}